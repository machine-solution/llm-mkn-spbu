# ДЗ3: Сравнение широкой и глубокой моделей Llama

## Задание

Запустить 2 обучения Llama с одинаковым числом параметров:
1. **Широкая Llama** (hidden_size увеличен)
2. **Глубокая Llama** (n_layers увеличен)

Снять профили по памяти и скорости, объяснить разницу в профилях.

## Конфигурации моделей

### Широкая модель
- n_layers: 1
- n_heads: 2
- hidden_size: 32 (широкие слои)
- vocab_size: 4096
- Параметров: ~279,040

### Глубокая модель  
- n_layers: 34
- n_heads: 2
- hidden_size: 16 (узкие слои)
- vocab_size: 4096
- Параметров: ~278,512

## Запуск

### Поиск параметров моделей
```bash
python hw3_find_params.py
```

### Полное обучение с профилированием
```bash
python hw3_train.py
```

### Обучение отдельных моделей
```bash
# Широкая модель
python train.py model=llama_wide hydra.run.dir=./logs_hw3/wide

# Глубокая модель  
python train.py model=llama_deep hydra.run.dir=./logs_hw3/deep
```

## Ожидаемые результаты

**Широкая модель** должна:
- Использовать больше памяти (большие матрицы в слоях)
- Быть быстрее в обучении (меньше слоев для прохода)

**Глубокая модель** должна:
- Использовать меньше памяти (меньшие матрицы в слоях)
- Быть медленнее в обучении (больше слоев для прохода)

## Файлы

- `conf/model/llama_wide.yaml` - конфигурация широкой модели
- `conf/model/llama_deep.yaml` - конфигурация глубокой модели
- `hw3_train.py` - основной скрипт для запуска обеих моделей с профилированием
- `hw3_find_params.py` - поиск конфигураций с одинаковым количеством параметров
