# @package _global_

training:
  batch_size: 64
  learning_rate: 1e-4
  num_epochs: 1
  warmup_steps: 10
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
  save_every: 1000
  log_every: 100
  
  # Optimizer
  optimizer: "adamw"
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  
  # Scheduler
  scheduler: "cosine"
  min_lr: 1e-6
